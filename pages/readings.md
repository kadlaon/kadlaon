[Sebastian Ruder's Thesis](http://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf) \
[Philipp Koehnn: Neural Machine Translation (Draft paper)](https://arxiv.org/pdf/1709.07809.pdf) \
[Graham Neubig: Neural Machine Translation and Sequence-to-sequence Models:A Tutorial](https://arxiv.org/pdf/1703.01619.pdf) \
[TensorFlow Dataset](https://www.tensorflow.org/datasets/overview) \
[Guide to pre-processing](https://www.kdnuggets.com/2018/08/practitioners-guide-processing-understanding-text-2.html) \
[On fixing github issues](https://github.community/t5/How-to-use-Git-and-GitHub/git-clone-is-not-working-for-a-private-repo/td-p/2513) \
[Word2Vec using numpy](https://github.com/DerekChia/word2vec_numpy) \
[FastText for Misspellings](https://ai.facebook.com/blog/-a-new-model-for-word-embeddings-that-are-resilient-to-misspellings-/) \
[Embedding Projector](https://medium.com/@aakashchotrani/visualizing-your-own-word-embeddings-using-tensorflow-688b3a7750ee) 

# About Corpus Linguistics
[1] [Why Chomsky was Wrong About Corpus Linguistics](https://corplingstats.wordpress.com/2016/11/02/why-chomsky-was-wrong/) \
[2] [Building a Wikipedia Text Corpus for Natural Language Processing](https://www.kdnuggets.com/2017/11/building-wikipedia-text-corpus-nlp.html) \
[3] [Developing Linguistic Corpora: A Guide to Good Practice Corpus and Text â€” Basic Principles](https://ota.ox.ac.uk/documents/creating/dlc/chapter1.htm) \
[4] [Resources for Philippine Languages: Collection, Annotation, and Modeling](https://www.aclweb.org/anthology/Y16-3015 (http://bit.ly/1MpcFoT))

# Neural Machine Translation
[1] Six Challenges of Neural Machine Translation : Six Challenges for Neural Machine Translation \
[2] [How does attention works in Encoder-Decoder RNN](https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/) \
[3] [Facebook AI Research Sequence-to-Sequence Toolkit written in Python](https://github.com/pytorch/fairseq)\
[4] [Recurrent Continous Translation Models](https://www.aclweb.org/anthology/D13-1176)\
[5] [A Teacher-Student Framework for Zero-Resource Neural Machine Translation](https://arxiv.org/pdf/1705.00753.pdf)\
[6] [Unsupervised Pivot Translation for Distant Languages](https://www.aclweb.org/anthology/P19-1017)\
[7] [On Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\
[8] [Fully Character-Level Neural Machine Translation without Explicit Segmentation](https://www.aclweb.org/anthology/Q17-1026)\
[9] [Intermediate Report: Morphologically Rich Languages](http://www.qt21.eu/wp-content/uploads/2017/07/QT21-D2.1-final.pdf)\
[10] [What is Neural Machine Translation](https://towardsdatascience.com/neural-machine-translation-15ecf6b0b)


Domain Adaptation for NMT
[1] [Multi-Domain Neural Machine Translation with Word-Level Domain Context Discrimination(https://www.aclweb.org/anthology/D18-1041) \
[2] [A Survey of Domain Adaptation for Neural Machine Translation](https://arxiv.org/pdf/1806.00258.pdf)
[3] [Evaluating Domain Adaptation for Machine Translation Across Scenarios](https://www.aclweb.org/anthology/L18-1002)

Morphological Analysis
[1] [Classification](https://machinelearnings.co/text-classification-using-neural-networks-f5cd7b8765c6)
[2] [Workshop on Subword and Character LEvel Models in NLP (2018)](https://aclweb.org/anthology/events/sclem-2018/)
[3] [Pushing the limits of low-resource morphological inflection] (https://arxiv.org/pdf/1908.05838.pdf)
[4] [Incorporating Latent Meanings of Morphological Compositions to Enhance Word Embeddings](https://www.aclweb.org/anthology/P18-1114)
[5] [Morphological Word Embeddings](https://arxiv.org/pdf/1907.02423.pdf)
[6] [Adapting Word Embeddings to New Languages with Morphological and Phonological Subword Representations]( http://www.cs.cmu.edu/~jgc/publication/Adapting%20Word%20Embeddings%20to%20New%20Languages%20with%20Morphological%20and%20Phonological%20Subword%20Representations.pdf)
[7] [Enriching Word Vectors with Subword Information](https://arxiv.org/pdf/1607.04606.pdf)
[8] [Meaningless yet meaningful](https://aclweb.org/anthology/W18-1207)
[9] [Compositional Representation of Morphologically-Rich Input for Neural Machine Translation](https://www.aclweb.org/anthology/P18-2049.pdf)
[10] [One Size Does Not Fit All: Comparing NMT Representations of Different Granularities](http://www.statmt.org/OSMOSES/naacl-19.pdf)
[11] [What do Neural Machine Translation Models Learn about Morphology?](http://statmt.org/OSMOSES/morphology17.pdf)
[12] [Compositional Morphology for Word Representations and Language Modelling](http://proceedings.mlr.press/v32/botha14.pdf)
[13] [Source of the NMT image](https://github.com/tensorflow/nmt/issues/231)
[14] [Morphological Word Embeddings for Arabic Neural Machine Translation in Low-Resource Settings](https://www.aclweb.org/anthology/W18-1201.pdf)


# Neural Network
[1] [INCREMENTAL LEARNING IN BIOLOGICAL AND MACHINE LEARNING SYSTEMS](https://pdfs.semanticscholar.org/49d4/4313f34edb97b55dfadb7cbe503adccccd17.pdf)
[2] [Recurrent Neural Network](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)


